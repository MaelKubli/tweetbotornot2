---
title: "Intro"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette provides a quick introduction to the tweetbotornot2 package. 
**Section 1** gives quick examples of the main functionality offered by 
this package. **Section 2** provides background and addresses some common issues
related to obtaining the proper authorization for interacting with Twitter's 
REST API. And **Section 3** offers some guidance (and an example) on how to 
collect bot estimates/Twitter data for a large number of users.

```{r setup}
## load package
library(tweetbotornot2)
```

## 1. Bot probability estimates

The main use case for {tweetbotornot} is to get bot-probability estimates for 
one or more Twitter accounts. This can be done with either `predict_bot()` or
`predict_bot_score()`. The former returns a `data.table` with user information
(user ID and screen name). The latter returns a numeric vector of estimates 
matched to the input order of users.

```{r, eval=FALSE}
## returns a data.table
predict_bot(c("netflix_bot", "PatrickMahomes"))

## returns a numeric vector
predict_bot_score(c("netflix_bot", "PatrickMahomes"))
```

One thing missing from the above code of course is the actual Twitter data used
to make the bot probability estimates. When screen names (or user IDs) are 
provided, tweetbotornot2 does that work (with the help of the [rtweet](https://rtweet.info) package)
behind the scenes. But in order for the rtweet package to work, users must 
obtain the proper authorization to interact with Twitter's REST API. For most
users this process is now a breeze–users can run the above code and if necessary
they will be prompted to authorize access to the embedded rtweet app. But for 
users working on remote servers or trying to generate estimates on thousands or
millions of Twitter uers, the next section will get into some of the relevant
details regarding API authorization.

## 2. Authentication

Use of this package typically<sup>1</sup> requires, at a minimum, (a) internet 
connection and (b) an active Twitter account–i.e., you will need to authorize
rtweet's *rstats2twitter* app to act (communicate from R to Twitter) on behalf 
of your Twitter account. If you're already signed into Twitter on your web 
browser, this can be done with a single click. Otherwise, you will have to sign 
in and *then* click.

### Saving/using Twitter API token

If you're working in an interactive R session on a local machine (in the RStudio 
application, for example), this process should be relatively straight forward.<sup>1</sup>
However, if you're working on a remote server or in the cloud (in RStudio Server,
for example), then it's a bit trickier. The easiest solution is to start a local, 
interactive session of R and then do the following:

1. **On a machine with a local version of R**, send API request via {rtweet}
   ```{r, eval=FALSE}
   rstats <- rtweet::search_tweets("rstats")
   ```
1. The above code actually creates (as a side effect) a Twitter token for you. So, on the same machine, save the token as an `.rds` file
   ```{r, eval=FALSE}
   saveRDS(rtweet::get_token(), "rtweet_token.rds")
   ```
1. Upload the saved token file to your **cloud/server**
1. Once the token is uploaded to the cloud, you can read the token file and 
specify the resulting object in each {tweetbotornot} call, e.g.,
   ```{r, eval=FALSE}
   token <- readRDS("rtweet_token.rds")
   predict_bot("jack", token = token)
   ```
   or set the path to the saved token as an R environment variable. The latter 
   option (example below) allows you to avoid having to specify the token each time.
   ```{r, eval=FALSE}
   Sys.setenv(TWITTER_PAT = "/path/to/rtweet_token.rds")
   predict_bot("jack")
   ```
   or set the R environment variable for current **and** future sessions (on a
   given machine), you can also try `tfse::set_renv()`.
   ```{r, eval=FALSE}
   tfse::set_renv(TWITTER_PAT = "/path/to/rtweet_token.rds")
   ```

### Rate limits

Assuming users haven't made any recent calls to Twitter's API, the predict functions
described earlier should work for up to 900 users at a time. For some users (e.g., those with a Twitter 
token created using credentials from their own Twitter application or tokens 
created at a time of sufficiently provided permissions by the embedded rtweet app)
it's possible to get estimates for up to 1,500 accounts at a time using Twitter's
application-only (bearer) token. Converting a normal user token into a bearer 
token requires an access level of `"read-write-directmessages"`. The code below 
should print the access level associated with your current token.

```{r, eval=FALSE}
## view access level of current token
print(rtweet:::api_access_level())
```

The drawback to the application-only authorization method is that your requests
will contain no information about the authenticating user. The upside to this
kind of token is access to a different set of rate limits. The different rate 
limits aren't always desirable (compared to the normal user token), but for the
some endpoints–like `users/timelines` (900 vs. 1,500 requests per 15 min.) and
`search/tweets` (18,000 vs. 45,000 requests per 15 min.)–the difference is quite 
significant.

If the output from the above code was `"read-write-directmessages"`, then users
can specify the bearer token via `rtweet::bearer_token()` for the `token` argument
in bot-predicting calls. The code below, for example, checks the bearer token rate
limit before and after executing `predict_bot()`.

```{r, eval=FALSE}
## check bearer token rate limit
rtweet::rate_limit(rtweet::bearer_token(), "get_timeline")

## get bot estimates for two users
predict_bot(c("netflix_bot", "PatrickMahomes"), token = rtweet::bearer_token())

## re-check bearer token rate limit
rtweet::rate_limit(rtweet::bearer_token(), "get_timeline")
```

## 3. Getting estimates for lots of users

### Example users

For example purposes, this vignette retrieves user IDs from accounts appearing
on a Twitter list of 2020 Democratic presidential candidates.

```{r, eval=FALSE}
## get accounts from the twitter list: twitter.com/kearneymw/lists/dems-pres-2020
d20 <- rtweet::lists_members(owner_user = "kearneymw", slug = "dems-pres-2020")

## store IDs as 'users' vector
users <- d20$user_id
```

### Making larger requests

To get estimates for hundreds or thousands of users, it is wise to use a for-loop 
and wait for the rate limit reset by sleeping between calls. This can be done in a number
of ways, but the functions `chunk_users()` and `chunk_users_data()` are provided
to make this easier. The code below should work for either user or bearer (rtweet)
token. It also prints out rate limit information and percent complete information.

```{r, eval=FALSE}
## convert users vector (user IDs or screen names) a list of 10-user chunks
users <- chunk_users(users, n = 10)

## initialize output vector
output <- vector("list", length(users))

for (i in seq_along(output)) {
  ## check rate limit- if < 10 calls remain, sleep until reset
  print(rl <- rtweet::rate_limit(query = "get_timelines"))
  while (rl[["remaining"]] < 10L) {
    cat("Sleeping for", round(max(as.numeric(rl[["reset"]], "mins"), 0.5), 1), "minutes...")
    Sys.sleep(max(as.numeric(rl[["reset"]], "secs"), 30))
    rl <- rtweet::rate_limit(query = "get_timelines")
  }
  
  ## get bot estimates
  output[[i]] <- predict_bot(users[[i]])
  
  ## print iteration update
  cat(sprintf("%d / %d (%.f%%)\n", i, length(output), i / length(output) * 100))
}

## merge into single data table
output <- do.call("rbind", output)

## sort by bot probability
output[order(-prob_bot), ]
```

If you'd also like to keep the raw (timeline) Twitter data, the above for loop
can be modified to collect the Twitter data prior to generating estimates with
`predict_bot()`. This is demonstrated in the code below.

```{r, eval=FALSE}
## convert users vector (user IDs or screen names) a list of 10-user chunks
users <- chunk_users(users, n = 10)

## initialize output vectors
output <- vector("list", length(users))
usrtml <- vector("list", length(users))

for (i in seq_along(output)) {
  ## check rate limit- if < 10 calls remain, sleep until reset
  print(rl <- rtweet::rate_limit(query = "get_timelines"))
  while (rl[["remaining"]] < 10L) {
    cat("Sleeping for", round(max(as.numeric(rl[["reset"]], "mins"), 0.5), 1), "minutes...")
    Sys.sleep(max(as.numeric(rl[["reset"]], "secs"), 30))
    rl <- rtweet::rate_limit(query = "get_timelines")
  }
  
  ## get user timeline data (set check=FALSE to avoid excessive rate-limit calls)
  usrtml[[i]] <- rtweet::get_timelines(users[[i]], n = 200, check = FALSE)
  
  ## use the timeline data to get bot estimates
  output[[i]] <- predict_bot(users[[i]])
  
  ## print iteration update
  cat(sprintf("%d / %d (%.f%%)\n", i, length(output), i / length(output) * 100))
}

## merge both data lists into single data tables
output <- do.call("rbind", output)
usrtml <- do.call("rbind", usrtml)

## sort by bot probability
output[order(-prob_bot), ]

## preview timeline data
usrtml[1:10, ]
```


<sup>1</sup> Unless you've already collected user timeline data, **[`{tweetbotornot2}`](tweetbotornot2.mikewk.com)** 
relies on [`{rtweet}`](https://rtweet.info) to pull data from [Twitter's REST API](https://developer.twitter.com/en/docs).

<sup>2</sup> The first time you make a request [from your computer] to Twitter's API's, a 
browser should pop-up, asking you to authorize the app. After that, the
authorization token is stored and remembered behind the scenes (you will only 
have to reauthorize the app if your Twitter settings or token credentials get 
reset or if you use a different machine).
